{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa as lr\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Data/genres_original/\"\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>blues.00000.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.350088</td>\n",
       "      <td>0.088757</td>\n",
       "      <td>0.130228</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>1784.165850</td>\n",
       "      <td>129774.064525</td>\n",
       "      <td>2002.449060</td>\n",
       "      <td>85882.761315</td>\n",
       "      <td>...</td>\n",
       "      <td>52.420910</td>\n",
       "      <td>-1.690215</td>\n",
       "      <td>36.524071</td>\n",
       "      <td>-0.408979</td>\n",
       "      <td>41.597103</td>\n",
       "      <td>-2.303523</td>\n",
       "      <td>55.062923</td>\n",
       "      <td>1.221291</td>\n",
       "      <td>46.936035</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>blues.00001.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.340914</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.095948</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>1530.176679</td>\n",
       "      <td>375850.073649</td>\n",
       "      <td>2039.036516</td>\n",
       "      <td>213843.755497</td>\n",
       "      <td>...</td>\n",
       "      <td>55.356403</td>\n",
       "      <td>-0.731125</td>\n",
       "      <td>60.314529</td>\n",
       "      <td>0.295073</td>\n",
       "      <td>48.120598</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>51.106190</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>45.786282</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>blues.00002.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>0.175570</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>1552.811865</td>\n",
       "      <td>156467.643368</td>\n",
       "      <td>1747.702312</td>\n",
       "      <td>76254.192257</td>\n",
       "      <td>...</td>\n",
       "      <td>40.598766</td>\n",
       "      <td>-7.729093</td>\n",
       "      <td>47.639427</td>\n",
       "      <td>-1.816407</td>\n",
       "      <td>52.382141</td>\n",
       "      <td>-3.439720</td>\n",
       "      <td>46.639660</td>\n",
       "      <td>-2.231258</td>\n",
       "      <td>30.573025</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>blues.00003.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.404785</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>1070.106615</td>\n",
       "      <td>184355.942417</td>\n",
       "      <td>1596.412872</td>\n",
       "      <td>166441.494769</td>\n",
       "      <td>...</td>\n",
       "      <td>44.427753</td>\n",
       "      <td>-3.319597</td>\n",
       "      <td>50.206673</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>37.319130</td>\n",
       "      <td>-0.619121</td>\n",
       "      <td>37.259739</td>\n",
       "      <td>-3.407448</td>\n",
       "      <td>31.949339</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>blues.00004.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.308526</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>1835.004266</td>\n",
       "      <td>343399.939274</td>\n",
       "      <td>1748.172116</td>\n",
       "      <td>88445.209036</td>\n",
       "      <td>...</td>\n",
       "      <td>86.099236</td>\n",
       "      <td>-5.454034</td>\n",
       "      <td>75.269707</td>\n",
       "      <td>-0.916874</td>\n",
       "      <td>53.613918</td>\n",
       "      <td>-4.404827</td>\n",
       "      <td>62.910812</td>\n",
       "      <td>-11.703234</td>\n",
       "      <td>55.195160</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>rock.00095.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.352063</td>\n",
       "      <td>0.080487</td>\n",
       "      <td>0.079486</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>2008.149458</td>\n",
       "      <td>282174.689224</td>\n",
       "      <td>2106.541053</td>\n",
       "      <td>88609.749506</td>\n",
       "      <td>...</td>\n",
       "      <td>45.050526</td>\n",
       "      <td>-13.289984</td>\n",
       "      <td>41.754955</td>\n",
       "      <td>2.484145</td>\n",
       "      <td>36.778877</td>\n",
       "      <td>-6.713265</td>\n",
       "      <td>54.866825</td>\n",
       "      <td>-1.193787</td>\n",
       "      <td>49.950665</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>rock.00096.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.398687</td>\n",
       "      <td>0.075086</td>\n",
       "      <td>0.076458</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>2006.843354</td>\n",
       "      <td>182114.709510</td>\n",
       "      <td>2068.942009</td>\n",
       "      <td>82426.016726</td>\n",
       "      <td>...</td>\n",
       "      <td>33.851742</td>\n",
       "      <td>-10.848309</td>\n",
       "      <td>39.395096</td>\n",
       "      <td>1.881229</td>\n",
       "      <td>32.010040</td>\n",
       "      <td>-7.461491</td>\n",
       "      <td>39.196327</td>\n",
       "      <td>-2.795338</td>\n",
       "      <td>31.773624</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>rock.00097.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.432142</td>\n",
       "      <td>0.075268</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>2077.526598</td>\n",
       "      <td>231657.968040</td>\n",
       "      <td>1927.293153</td>\n",
       "      <td>74717.124394</td>\n",
       "      <td>...</td>\n",
       "      <td>33.597008</td>\n",
       "      <td>-12.845291</td>\n",
       "      <td>36.367264</td>\n",
       "      <td>3.440978</td>\n",
       "      <td>36.001110</td>\n",
       "      <td>-12.588070</td>\n",
       "      <td>42.502201</td>\n",
       "      <td>-2.106337</td>\n",
       "      <td>29.865515</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>rock.00098.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.362485</td>\n",
       "      <td>0.091506</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>1398.699344</td>\n",
       "      <td>240318.731073</td>\n",
       "      <td>1818.450280</td>\n",
       "      <td>109090.207161</td>\n",
       "      <td>...</td>\n",
       "      <td>46.324894</td>\n",
       "      <td>-4.416050</td>\n",
       "      <td>43.583942</td>\n",
       "      <td>1.556207</td>\n",
       "      <td>34.331261</td>\n",
       "      <td>-5.041897</td>\n",
       "      <td>47.227180</td>\n",
       "      <td>-3.590644</td>\n",
       "      <td>41.299088</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>rock.00099.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.358401</td>\n",
       "      <td>0.085884</td>\n",
       "      <td>0.054454</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>1609.795082</td>\n",
       "      <td>422203.216152</td>\n",
       "      <td>1797.213044</td>\n",
       "      <td>120115.632927</td>\n",
       "      <td>...</td>\n",
       "      <td>59.167755</td>\n",
       "      <td>-7.069775</td>\n",
       "      <td>73.760391</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>76.504326</td>\n",
       "      <td>-2.025783</td>\n",
       "      <td>72.189316</td>\n",
       "      <td>1.155239</td>\n",
       "      <td>49.662510</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0    blues.00000.wav  661794          0.350088         0.088757  0.130228   \n",
       "1    blues.00001.wav  661794          0.340914         0.094980  0.095948   \n",
       "2    blues.00002.wav  661794          0.363637         0.085275  0.175570   \n",
       "3    blues.00003.wav  661794          0.404785         0.093999  0.141093   \n",
       "4    blues.00004.wav  661794          0.308526         0.087841  0.091529   \n",
       "..               ...     ...               ...              ...       ...   \n",
       "994   rock.00095.wav  661794          0.352063         0.080487  0.079486   \n",
       "995   rock.00096.wav  661794          0.398687         0.075086  0.076458   \n",
       "996   rock.00097.wav  661794          0.432142         0.075268  0.081651   \n",
       "997   rock.00098.wav  661794          0.362485         0.091506  0.083860   \n",
       "998   rock.00099.wav  661794          0.358401         0.085884  0.054454   \n",
       "\n",
       "      rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0    0.002827             1784.165850          129774.064525   \n",
       "1    0.002373             1530.176679          375850.073649   \n",
       "2    0.002746             1552.811865          156467.643368   \n",
       "3    0.006346             1070.106615          184355.942417   \n",
       "4    0.002303             1835.004266          343399.939274   \n",
       "..        ...                     ...                    ...   \n",
       "994  0.000345             2008.149458          282174.689224   \n",
       "995  0.000588             2006.843354          182114.709510   \n",
       "996  0.000322             2077.526598          231657.968040   \n",
       "997  0.001211             1398.699344          240318.731073   \n",
       "998  0.000336             1609.795082          422203.216152   \n",
       "\n",
       "     spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0                2002.449060            85882.761315  ...   52.420910   \n",
       "1                2039.036516           213843.755497  ...   55.356403   \n",
       "2                1747.702312            76254.192257  ...   40.598766   \n",
       "3                1596.412872           166441.494769  ...   44.427753   \n",
       "4                1748.172116            88445.209036  ...   86.099236   \n",
       "..                       ...                     ...  ...         ...   \n",
       "994              2106.541053            88609.749506  ...   45.050526   \n",
       "995              2068.942009            82426.016726  ...   33.851742   \n",
       "996              1927.293153            74717.124394  ...   33.597008   \n",
       "997              1818.450280           109090.207161  ...   46.324894   \n",
       "998              1797.213044           120115.632927  ...   59.167755   \n",
       "\n",
       "     mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
       "0      -1.690215   36.524071    -0.408979   41.597103    -2.303523   \n",
       "1      -0.731125   60.314529     0.295073   48.120598    -0.283518   \n",
       "2      -7.729093   47.639427    -1.816407   52.382141    -3.439720   \n",
       "3      -3.319597   50.206673     0.636965   37.319130    -0.619121   \n",
       "4      -5.454034   75.269707    -0.916874   53.613918    -4.404827   \n",
       "..           ...         ...          ...         ...          ...   \n",
       "994   -13.289984   41.754955     2.484145   36.778877    -6.713265   \n",
       "995   -10.848309   39.395096     1.881229   32.010040    -7.461491   \n",
       "996   -12.845291   36.367264     3.440978   36.001110   -12.588070   \n",
       "997    -4.416050   43.583942     1.556207   34.331261    -5.041897   \n",
       "998    -7.069775   73.760391     0.028346   76.504326    -2.025783   \n",
       "\n",
       "     mfcc19_var  mfcc20_mean  mfcc20_var  label  \n",
       "0     55.062923     1.221291   46.936035  blues  \n",
       "1     51.106190     0.531217   45.786282  blues  \n",
       "2     46.639660    -2.231258   30.573025  blues  \n",
       "3     37.259739    -3.407448   31.949339  blues  \n",
       "4     62.910812   -11.703234   55.195160  blues  \n",
       "..          ...          ...         ...    ...  \n",
       "994   54.866825    -1.193787   49.950665   rock  \n",
       "995   39.196327    -2.795338   31.773624   rock  \n",
       "996   42.502201    -2.106337   29.865515   rock  \n",
       "997   47.227180    -3.590644   41.299088   rock  \n",
       "998   72.189316     1.155239   49.662510   rock  \n",
       "\n",
       "[999 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/features_extraction.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "    \n",
    "    a = (xx - h) // 2\n",
    "    aa = xx - a - h\n",
    "\n",
    "    b = (yy - w) // 2\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df_in):\n",
    "    features=[] \n",
    "    labels=[] \n",
    "    for index in range(0,len(df_in)):     \n",
    "      filename = df_in.iloc[index]['filename']  \n",
    "      tstart = 0 \n",
    "      tend = df_in.iloc[index]['length'] \n",
    "      species_id = df_in.iloc[index]['label']         \n",
    "      y, sr = librosa.load('Data/genres_original/'+filename,sr=22050)\n",
    "      y_cut = y[round(tstart*sr,ndigits=None)\n",
    "         :round(tend*sr, ndigits= None)]\n",
    "      data = np.array([padding(librosa.feature.mfcc(y_cut, \n",
    "         n_fft=255,hop_length=512,n_mfcc=128),130,1500)])\n",
    "      features.append(data)\n",
    "      labels.append(species_id)\n",
    "    output=np.concatenate(features,axis=0)\n",
    "    return(np.array(output), labels)\n",
    "X,y=get_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array((X-np.min(X))/(np.max(X)-np.min(X)))\n",
    "X = X/np.std(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "names_list = y\n",
    "con = LabelEncoder()\n",
    "y = con.fit_transform(names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((561, 130, 1500), (250, 130, 1500), (188, 130, 1500), 561, 250, 188)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split twice to get the validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=123)\n",
    "#Print the shapes\n",
    "X_train.shape, X_test.shape, X_val.shape, len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               834048    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 837,144\n",
      "Trainable params: 837,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(130,1500)\n",
    "model = tensorflow.keras.Sequential()\n",
    "model.add(LSTM(128,input_shape=input_shape))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - ETA: 23s - loss: 3.5331 - acc: 0.0000e+ - ETA: 4s - loss: 3.3263 - acc: 0.0486     - ETA: 3s - loss: 3.2023 - acc: 0.064 - ETA: 2s - loss: 3.1676 - acc: 0.059 - ETA: 2s - loss: 3.1235 - acc: 0.063 - ETA: 1s - loss: 3.0990 - acc: 0.074 - ETA: 0s - loss: 3.0610 - acc: 0.085 - ETA: 0s - loss: 3.0357 - acc: 0.087 - 10s 926ms/step - loss: 3.0357 - acc: 0.0873 - val_loss: 2.7684 - val_acc: 0.1170\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.7426 - acc: 0.083 - ETA: 4s - loss: 2.7497 - acc: 0.090 - ETA: 3s - loss: 2.7267 - acc: 0.106 - ETA: 2s - loss: 2.7358 - acc: 0.114 - ETA: 2s - loss: 2.7248 - acc: 0.116 - ETA: 1s - loss: 2.7218 - acc: 0.120 - ETA: 0s - loss: 2.7108 - acc: 0.113 - ETA: 0s - loss: 2.7016 - acc: 0.112 - 7s 864ms/step - loss: 2.7016 - acc: 0.1123 - val_loss: 2.6183 - val_acc: 0.0851\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.5917 - acc: 0.166 - ETA: 4s - loss: 2.5960 - acc: 0.145 - ETA: 3s - loss: 2.5780 - acc: 0.152 - ETA: 2s - loss: 2.5863 - acc: 0.131 - ETA: 2s - loss: 2.5816 - acc: 0.130 - ETA: 1s - loss: 2.5779 - acc: 0.131 - ETA: 0s - loss: 2.5728 - acc: 0.123 - ETA: 0s - loss: 2.5659 - acc: 0.119 - 6s 820ms/step - loss: 2.5659 - acc: 0.1194 - val_loss: 2.5145 - val_acc: 0.0798\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.5180 - acc: 0.027 - ETA: 4s - loss: 2.5085 - acc: 0.076 - ETA: 3s - loss: 2.4898 - acc: 0.097 - ETA: 2s - loss: 2.4856 - acc: 0.107 - ETA: 2s - loss: 2.4815 - acc: 0.111 - ETA: 1s - loss: 2.4763 - acc: 0.115 - ETA: 0s - loss: 2.4753 - acc: 0.109 - ETA: 0s - loss: 2.4709 - acc: 0.107 - 7s 835ms/step - loss: 2.4709 - acc: 0.1070 - val_loss: 2.4533 - val_acc: 0.0798\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.4627 - acc: 0.027 - ETA: 4s - loss: 2.4495 - acc: 0.076 - ETA: 3s - loss: 2.4349 - acc: 0.097 - ETA: 2s - loss: 2.4286 - acc: 0.107 - ETA: 2s - loss: 2.4268 - acc: 0.111 - ETA: 1s - loss: 2.4222 - acc: 0.115 - ETA: 0s - loss: 2.4223 - acc: 0.109 - ETA: 0s - loss: 2.4189 - acc: 0.107 - 7s 831ms/step - loss: 2.4189 - acc: 0.1070 - val_loss: 2.4171 - val_acc: 0.0798\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.4275 - acc: 0.027 - ETA: 4s - loss: 2.4129 - acc: 0.062 - ETA: 3s - loss: 2.4023 - acc: 0.078 - ETA: 3s - loss: 2.3955 - acc: 0.097 - ETA: 2s - loss: 2.3955 - acc: 0.094 - ETA: 1s - loss: 2.3914 - acc: 0.094 - ETA: 0s - loss: 2.3917 - acc: 0.093 - ETA: 0s - loss: 2.3888 - acc: 0.094 - 8s 999ms/step - loss: 2.3888 - acc: 0.0945 - val_loss: 2.3955 - val_acc: 0.0851\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - ETA: 6s - loss: 2.4054 - acc: 0.111 - ETA: 4s - loss: 2.3901 - acc: 0.104 - ETA: 3s - loss: 2.3824 - acc: 0.106 - ETA: 2s - loss: 2.3760 - acc: 0.118 - ETA: 2s - loss: 2.3767 - acc: 0.111 - ETA: 1s - loss: 2.3729 - acc: 0.108 - ETA: 0s - loss: 2.3732 - acc: 0.105 - ETA: 0s - loss: 2.3707 - acc: 0.105 - 7s 839ms/step - loss: 2.3707 - acc: 0.1052 - val_loss: 2.3812 - val_acc: 0.0851\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3908 - acc: 0.111 - ETA: 4s - loss: 2.3752 - acc: 0.104 - ETA: 3s - loss: 2.3688 - acc: 0.106 - ETA: 2s - loss: 2.3632 - acc: 0.118 - ETA: 2s - loss: 2.3642 - acc: 0.111 - ETA: 1s - loss: 2.3604 - acc: 0.108 - ETA: 0s - loss: 2.3608 - acc: 0.105 - ETA: 0s - loss: 2.3586 - acc: 0.105 - 7s 830ms/step - loss: 2.3586 - acc: 0.1052 - val_loss: 2.3710 - val_acc: 0.0851\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3805 - acc: 0.111 - ETA: 4s - loss: 2.3651 - acc: 0.104 - ETA: 3s - loss: 2.3590 - acc: 0.106 - ETA: 2s - loss: 2.3539 - acc: 0.118 - ETA: 2s - loss: 2.3549 - acc: 0.111 - ETA: 1s - loss: 2.3514 - acc: 0.108 - ETA: 0s - loss: 2.3519 - acc: 0.105 - ETA: 0s - loss: 2.3499 - acc: 0.105 - 6s 819ms/step - loss: 2.3499 - acc: 0.1052 - val_loss: 2.3635 - val_acc: 0.0904\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3727 - acc: 0.055 - ETA: 4s - loss: 2.3578 - acc: 0.083 - ETA: 3s - loss: 2.3515 - acc: 0.074 - ETA: 2s - loss: 2.3468 - acc: 0.093 - ETA: 2s - loss: 2.3479 - acc: 0.091 - ETA: 1s - loss: 2.3444 - acc: 0.092 - ETA: 0s - loss: 2.3452 - acc: 0.089 - ETA: 0s - loss: 2.3435 - acc: 0.089 - 6s 824ms/step - loss: 2.3435 - acc: 0.0891 - val_loss: 2.3577 - val_acc: 0.0798\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3666 - acc: 0.027 - ETA: 4s - loss: 2.3524 - acc: 0.076 - ETA: 3s - loss: 2.3458 - acc: 0.097 - ETA: 2s - loss: 2.3412 - acc: 0.107 - ETA: 2s - loss: 2.3423 - acc: 0.111 - ETA: 1s - loss: 2.3391 - acc: 0.115 - ETA: 0s - loss: 2.3401 - acc: 0.109 - ETA: 0s - loss: 2.3385 - acc: 0.107 - 6s 824ms/step - loss: 2.3385 - acc: 0.1070 - val_loss: 2.3531 - val_acc: 0.0798\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3617 - acc: 0.027 - ETA: 4s - loss: 2.3480 - acc: 0.076 - ETA: 3s - loss: 2.3414 - acc: 0.097 - ETA: 2s - loss: 2.3367 - acc: 0.107 - ETA: 2s - loss: 2.3380 - acc: 0.111 - ETA: 1s - loss: 2.3349 - acc: 0.115 - ETA: 0s - loss: 2.3360 - acc: 0.109 - ETA: 0s - loss: 2.3345 - acc: 0.107 - 6s 821ms/step - loss: 2.3345 - acc: 0.1070 - val_loss: 2.3494 - val_acc: 0.0798\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3575 - acc: 0.027 - ETA: 4s - loss: 2.3443 - acc: 0.076 - ETA: 3s - loss: 2.3378 - acc: 0.097 - ETA: 2s - loss: 2.3332 - acc: 0.107 - ETA: 2s - loss: 2.3345 - acc: 0.102 - ETA: 1s - loss: 2.3315 - acc: 0.108 - ETA: 0s - loss: 2.3327 - acc: 0.103 - ETA: 0s - loss: 2.3312 - acc: 0.101 - 6s 815ms/step - loss: 2.3312 - acc: 0.1016 - val_loss: 2.3462 - val_acc: 0.0798\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3541 - acc: 0.027 - ETA: 4s - loss: 2.3411 - acc: 0.076 - ETA: 3s - loss: 2.3348 - acc: 0.097 - ETA: 2s - loss: 2.3304 - acc: 0.107 - ETA: 2s - loss: 2.3317 - acc: 0.102 - ETA: 1s - loss: 2.3288 - acc: 0.101 - ETA: 0s - loss: 2.3299 - acc: 0.097 - ETA: 0s - loss: 2.3285 - acc: 0.096 - 6s 826ms/step - loss: 2.3285 - acc: 0.0963 - val_loss: 2.3436 - val_acc: 0.0798\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3514 - acc: 0.027 - ETA: 4s - loss: 2.3385 - acc: 0.076 - ETA: 3s - loss: 2.3324 - acc: 0.097 - ETA: 2s - loss: 2.3280 - acc: 0.107 - ETA: 2s - loss: 2.3294 - acc: 0.102 - ETA: 1s - loss: 2.3265 - acc: 0.101 - ETA: 0s - loss: 2.3276 - acc: 0.097 - ETA: 0s - loss: 2.3262 - acc: 0.096 - 7s 828ms/step - loss: 2.3262 - acc: 0.0963 - val_loss: 2.3414 - val_acc: 0.0798\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3491 - acc: 0.027 - ETA: 4s - loss: 2.3363 - acc: 0.076 - ETA: 3s - loss: 2.3304 - acc: 0.097 - ETA: 2s - loss: 2.3260 - acc: 0.107 - ETA: 2s - loss: 2.3274 - acc: 0.102 - ETA: 1s - loss: 2.3245 - acc: 0.101 - ETA: 0s - loss: 2.3257 - acc: 0.097 - ETA: 0s - loss: 2.3243 - acc: 0.096 - 7s 831ms/step - loss: 2.3243 - acc: 0.0963 - val_loss: 2.3395 - val_acc: 0.0798\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3472 - acc: 0.027 - ETA: 4s - loss: 2.3345 - acc: 0.076 - ETA: 3s - loss: 2.3286 - acc: 0.097 - ETA: 2s - loss: 2.3243 - acc: 0.107 - ETA: 2s - loss: 2.3257 - acc: 0.102 - ETA: 1s - loss: 2.3228 - acc: 0.101 - ETA: 0s - loss: 2.3239 - acc: 0.097 - ETA: 0s - loss: 2.3226 - acc: 0.096 - 6s 815ms/step - loss: 2.3226 - acc: 0.0963 - val_loss: 2.3380 - val_acc: 0.0798\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3456 - acc: 0.027 - ETA: 4s - loss: 2.3330 - acc: 0.076 - ETA: 3s - loss: 2.3270 - acc: 0.097 - ETA: 2s - loss: 2.3227 - acc: 0.107 - ETA: 2s - loss: 2.3241 - acc: 0.102 - ETA: 1s - loss: 2.3213 - acc: 0.101 - ETA: 0s - loss: 2.3225 - acc: 0.097 - ETA: 0s - loss: 2.3211 - acc: 0.096 - 6s 818ms/step - loss: 2.3211 - acc: 0.0963 - val_loss: 2.3366 - val_acc: 0.0798\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3442 - acc: 0.027 - ETA: 4s - loss: 2.3316 - acc: 0.076 - ETA: 3s - loss: 2.3257 - acc: 0.097 - ETA: 2s - loss: 2.3214 - acc: 0.107 - ETA: 2s - loss: 2.3228 - acc: 0.102 - ETA: 1s - loss: 2.3200 - acc: 0.108 - ETA: 0s - loss: 2.3211 - acc: 0.103 - ETA: 0s - loss: 2.3198 - acc: 0.101 - 6s 827ms/step - loss: 2.3198 - acc: 0.1016 - val_loss: 2.3353 - val_acc: 0.0798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3429 - acc: 0.027 - ETA: 4s - loss: 2.3303 - acc: 0.076 - ETA: 4s - loss: 2.3244 - acc: 0.097 - ETA: 3s - loss: 2.3201 - acc: 0.107 - ETA: 2s - loss: 2.3216 - acc: 0.102 - ETA: 1s - loss: 2.3188 - acc: 0.108 - ETA: 0s - loss: 2.3200 - acc: 0.103 - ETA: 0s - loss: 2.3187 - acc: 0.101 - 7s 933ms/step - loss: 2.3187 - acc: 0.1016 - val_loss: 2.3342 - val_acc: 0.0798\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3417 - acc: 0.027 - ETA: 4s - loss: 2.3292 - acc: 0.076 - ETA: 3s - loss: 2.3233 - acc: 0.097 - ETA: 2s - loss: 2.3191 - acc: 0.107 - ETA: 2s - loss: 2.3205 - acc: 0.102 - ETA: 1s - loss: 2.3177 - acc: 0.108 - ETA: 0s - loss: 2.3189 - acc: 0.103 - ETA: 0s - loss: 2.3177 - acc: 0.101 - 7s 818ms/step - loss: 2.3177 - acc: 0.1016 - val_loss: 2.3332 - val_acc: 0.0798\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3407 - acc: 0.027 - ETA: 5s - loss: 2.3282 - acc: 0.076 - ETA: 3s - loss: 2.3224 - acc: 0.097 - ETA: 3s - loss: 2.3181 - acc: 0.107 - ETA: 2s - loss: 2.3196 - acc: 0.102 - ETA: 1s - loss: 2.3168 - acc: 0.108 - ETA: 0s - loss: 2.3180 - acc: 0.103 - ETA: 0s - loss: 2.3167 - acc: 0.101 - 7s 886ms/step - loss: 2.3167 - acc: 0.1016 - val_loss: 2.3323 - val_acc: 0.0798\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3397 - acc: 0.027 - ETA: 3s - loss: 2.3273 - acc: 0.076 - ETA: 3s - loss: 2.3215 - acc: 0.097 - ETA: 2s - loss: 2.3172 - acc: 0.107 - ETA: 1s - loss: 2.3187 - acc: 0.102 - ETA: 1s - loss: 2.3159 - acc: 0.108 - ETA: 0s - loss: 2.3171 - acc: 0.103 - ETA: 0s - loss: 2.3159 - acc: 0.101 - 7s 859ms/step - loss: 2.3159 - acc: 0.1016 - val_loss: 2.3315 - val_acc: 0.0798\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3388 - acc: 0.027 - ETA: 4s - loss: 2.3265 - acc: 0.076 - ETA: 4s - loss: 2.3207 - acc: 0.097 - ETA: 3s - loss: 2.3164 - acc: 0.107 - ETA: 2s - loss: 2.3179 - acc: 0.111 - ETA: 1s - loss: 2.3152 - acc: 0.115 - ETA: 0s - loss: 2.3164 - acc: 0.109 - ETA: 0s - loss: 2.3151 - acc: 0.107 - 8s 1s/step - loss: 2.3151 - acc: 0.1070 - val_loss: 2.3307 - val_acc: 0.0798\n",
      "Epoch 25/50\n",
      "7/8 [=========================>....] - ETA: 5s - loss: 2.3381 - acc: 0.027 - ETA: 4s - loss: 2.3257 - acc: 0.076 - ETA: 4s - loss: 2.3199 - acc: 0.097 - ETA: 3s - loss: 2.3157 - acc: 0.107 - ETA: 2s - loss: 2.3172 - acc: 0.111 - ETA: 1s - loss: 2.3145 - acc: 0.115 - ETA: 0s - loss: 2.3157 - acc: 0.1091"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fbb1170ad961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n\u001b[1;32m----> 2\u001b[1;33m                     validation_data=(X_val, y_val), shuffle=False)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                    validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df.iloc[1,1:-1])\n",
    "x = np.asarray(x).astype('float')\n",
    "x = x[np.newaxis, :]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(df.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "for name in df.iloc[:,0]:\n",
    "    audio_file = glob(data_dir + name)\n",
    "    audio,sfreq = lr.load(audio_file[0],sr=sr)\n",
    "    mfccs.append(addmfcc(audio))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mfccs = {}\n",
    "for i in range(20):\n",
    "    new_mfccs['mfcc'+str(i+1)] = [mfccs[j][i] for j in range(len(mfccs))]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
